{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised GANの構造\n",
    "- 原理：GANでデータ分布への理解を利用して、少ないlabeled training dataだけど高い正確率分類器を得られる。\n",
    " - **internalizing hidden structures** in the data, semi-supervised learning strives to **generalize** from the **small subset of labeled** data points to effectively classify new, previously unseen examples.\n",
    " - The Generator serves as a source of additional information that **helps the Discriminator learn the relevant patterns** in the data.\n",
    "- semi-supervised learningは人の学習方法に近い。\n",
    "- 普段はGeneratorを重要視するが、ここでDiscriminatorを関心している。\n",
    "- labeled dataは良くtraining dataの1~2%しかない。\n",
    "- supervised部分：\n",
    " - Labeled data (x,y)はclass 1~nにトレーニング。（multiclass,例えばsoftmax）\n",
    "- unsupervised部分：\n",
    " - 他の部分は以前のGANと同じ。Unlabeled data xやGeneratorが作ったdata x* はtrue/fakeにトレーニング。（binary,例えばsigmoid）\n",
    "<img src=\"img/semi-supervised-gan-2020-02-25 20-31-50.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import (Activation, BatchNormalization, Concatenate, Dense, Dropout, \n",
    "                          Flatten, Input, Lambda, Reshape)\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "# one-hot encoding\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, num_labeled):\n",
    "        # labeled training dataの数\n",
    "        self.num_labeled = num_labeled\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = mnist.load_data()\n",
    "        \n",
    "        def preprocess_imgs(x):\n",
    "            x = (x.astype(np.float32) - 127.5) / 127.5\n",
    "            x = np.expand_dims(x, axis=3)\n",
    "            return x\n",
    "        \n",
    "        def preprocess_labels(y):\n",
    "            # -1の意味はこのdimensionのサイズが分からない。1列にしてaxis=0のサイズは自動的に決める。\n",
    "            return y.reshape(-1, 1)\n",
    "        \n",
    "        self.x_train = preprocess_imgs(self.x_train)\n",
    "        self.y_train = preprocess_labels(self.y_train)\n",
    "        \n",
    "        self.x_test = preprocess_imgs(self.x_test)\n",
    "        self.y_test = preprocess_labels(self.y_test)\n",
    "        \n",
    "    def batch_labeled(self, batch_size):\n",
    "        idx = np.random.randint(0, self.num_labeled, batch_size)\n",
    "        imgs = self.x_train[idx]\n",
    "        labels = self.y_train[idx]\n",
    "        return imgs, labels\n",
    "    \n",
    "    def batch_unlabeled(self, batch_size):\n",
    "        idx = np.random.randint(self.num_labeled, self.x_train.shape[0], batch_size)\n",
    "        imgs = self.x_train[idx]\n",
    "        return imgs\n",
    "    \n",
    "    def training_set(self):\n",
    "        x_train = self.x_train[range(self.num_labeled)]\n",
    "        y_train = self.y_train[range(self.num_labeled)]\n",
    "        return x_train, y_train\n",
    "    \n",
    "    def test_set(self):\n",
    "        return self.x_test, self.y_test\n",
    "    \n",
    "    def training_set_num(self, number_of_samples):\n",
    "        x_train = self.x_train[range(number_of_samples)]\n",
    "        y_train = self.y_train[range(number_of_samples)]\n",
    "        return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100個labeled training dataを使う\n",
    "num_labeled = 100\n",
    "dataset = Dataset(num_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "z_dim = 100\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generator\n",
    "- DCGANと変わらない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(z_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256 * 7 * 7, input_dim=z_dim))\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same')) # 14*14*128\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same')) # 14*14*64\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same')) # 28*28*1\n",
    "    model.add(Activation('tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Discriminator\n",
    "- 最後のActivationが違って、その前のネットワークは一緒。\n",
    "- 最後のActivationは、supervised multiclass用だったら、softmax。unsupervised binary用だったら、sigmoid.\n",
    "- Dropoutを使う理由：Generalizationが良くなるように。\n",
    " - We add dropout because of the increased complexity of the SGAN classification task and to improve the model's ability to **generalize from only 100 labeled examples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_net(img_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding='same')) # 14*14*32\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same')) # 7*7*64\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same')) # 3*3*128\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    # Full connected layer with num_classes neurons\n",
    "    model.add(Dense(num_classes))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_supervised(discriminator_net):\n",
    "    # 直接にdiscriminator_netの後ろにaddしてもいいでしょう。多分modelを新規する理由は、discriminatorを変更したくないから。\n",
    "    model = Sequential()\n",
    "    model.add(discriminator_net)\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $sigmoid=\\frac{1}{1+e^{-x}}=\\frac{e^x}{e^x+1}=1-\\frac{1}{1+e^x}$.\n",
    "- 下記実装はActivation('sigmoid')を使ってない。理由は10 * 1のvectorではなく、scalarが欲しいからです。\n",
    " - Actiとpredictの差はK.sumだ。Activation('sigmoid')の結果はnum_classes * 1です。predictの結果はscalarです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator_unsupervised(discriminator_net):\n",
    "    model = Sequential()\n",
    "    model.add(discriminator_net)\n",
    "    # sigmoid function\n",
    "    def predict(x):\n",
    "        prediction = 1.0 - (1.0 /\n",
    "                           (K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\n",
    "        return prediction\n",
    "    \n",
    "    model.add(Lambda(predict))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Supervised & Unsupervised Discriminatorのcompile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Discriminator network: These layers are shared during supervised and unsupervised training\n",
    "discriminator_net = build_discriminator_net(img_shape)\n",
    "\n",
    "discriminator_supervised = build_discriminator_supervised(discriminator_net)\n",
    "discriminator_supervised.compile(loss='categorical_crossentropy',\n",
    "                                metrics=['accuracy'],\n",
    "                                optimizer=Adam())\n",
    "\n",
    "discriminator_unsupervised = build_discriminator_unsupervised(discriminator_net)\n",
    "# unsupervised discriminatorでmetricsを使ってなく、これはただ後ほどのplotで使わないからだけ。\n",
    "discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
    "                                  optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: GANのcompile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator(z_dim)\n",
    "discriminator_unsupervised.trainable = False\n",
    "gan = build_gan(generator, discriminator_unsupervised)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training\n",
    "- トレーニンぐの順番はSupervised Discriminator, Unsupervised Discriminator, GAN.\n",
    "- みんなのbatch sizeは一緒になっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_losses = []\n",
    "iteration_checkpoints = []\n",
    "\n",
    "def train(iterations, batch_size, sample_interval):\n",
    "    real = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        # Discriminatorトレーニンぐ\n",
    "        imgs, labels = dataset.batch_labeled(batch_size)\n",
    "        # labelsをone-hotにする。ここはnum_classesがなくてもいいでしょう。\n",
    "        # labelの種類とnum_classesが合わなければどうなる？下を参考。\n",
    "        labels = to_categorical(labels, num_classes=num_classes)\n",
    "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
    "        z = np.random.normal(0.0, 1.0, (batch_size, z_dim))\n",
    "        gen_imgs = generator.predict(z)\n",
    "        \n",
    "        d_loss_supervised, accuracy = discriminator_supervised.train_on_batch(imgs, labels)\n",
    "        d_loss_real = discriminator_unsupervised.train_on_batch(imgs_unlabeled, real)\n",
    "        d_loss_fake = discriminator_unsupervised.train_on_batch(gen_imgs, fake)\n",
    "        d_loss_unsupervised = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # GANトレーニンぐ\n",
    "        z = np.random.normal(0.0, 1.0, (batch_size, z_dim))\n",
    "        g_loss = gan.train_on_batch(z, real)\n",
    "        if (iteration + 1) % sample_interval == 0:\n",
    "            supervised_losses.append(d_loss_supervised)\n",
    "            iteration_checkpoints.append(iteration + 1)\n",
    "            print(\"%d [D loss supervised: %.4f, acc.:%.2f%%] [D loss unsupervised: %.4f] [G loss: %f]\"\n",
    "                 % (iteration + 1, d_loss_supervised, 100 * accuracy, d_loss_unsupervised, g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = np.array([0,2,1,2,0])\n",
    "to_categorical(test_labels,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8207e81dca2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "to_categorical(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 [D loss supervised: 0.0117, acc.:100.00%] [D loss unsupervised: 0.1366] [G loss: 3.516179]\n",
      "1600 [D loss supervised: 0.0041, acc.:100.00%] [D loss unsupervised: 0.0352] [G loss: 2.798571]\n",
      "2400 [D loss supervised: 0.0046, acc.:100.00%] [D loss unsupervised: 0.1765] [G loss: 4.977440]\n",
      "3200 [D loss supervised: 0.0010, acc.:100.00%] [D loss unsupervised: 0.0878] [G loss: 4.970886]\n",
      "4000 [D loss supervised: 0.0002, acc.:100.00%] [D loss unsupervised: 0.4855] [G loss: 2.926418]\n",
      "4800 [D loss supervised: 0.0001, acc.:100.00%] [D loss unsupervised: 0.0976] [G loss: 5.711957]\n",
      "5600 [D loss supervised: 0.0001, acc.:100.00%] [D loss unsupervised: 0.3410] [G loss: 3.132472]\n",
      "6400 [D loss supervised: 0.0002, acc.:100.00%] [D loss unsupervised: 0.2417] [G loss: 5.609678]\n",
      "7200 [D loss supervised: 0.0001, acc.:100.00%] [D loss unsupervised: 0.0832] [G loss: 4.323412]\n",
      "8000 [D loss supervised: 0.0002, acc.:100.00%] [D loss unsupervised: 0.2228] [G loss: 5.358171]\n"
     ]
    }
   ],
   "source": [
    "iterations = 8000\n",
    "batch_size = 32\n",
    "sample_interval = 800\n",
    "train(iterations=iterations, batch_size=batch_size, sample_interval=sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_supervised.save_weights('sgan_discriminator_supervised.h5')\n",
    "gan.save_weights('sgan.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGAN ClassifierのTest Accuracy\n",
    "- SGANのtraining accuracyは100%ですが、generalizationはどう？training accuracyは意味少ない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(classifier):\n",
    "    x, y = dataset.test_set()\n",
    "    y = to_categorical(y, num_classes=num_classes)\n",
    "    _, accuracy = classifier.evaluate(x, y)\n",
    "    print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 40us/step\n",
      "Test Accuracy: 87.26%\n"
     ]
    }
   ],
   "source": [
    "evaluate_test(discriminator_supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-Supervised Classifier\n",
    "- SGANのsupervised Discriminatorと同じ構造を使う。\n",
    "- 同じく100個samplesでトレーニンぐすると、だいぶ差が出ている。30%。\n",
    "- 完全Supervisedは500個samplesを見ると、大体同じ正確率になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fully_supervised(number_of_samples):\n",
    "    mnist_classifier = build_discriminator_supervised(build_discriminator_net(img_shape))\n",
    "    mnist_classifier.compile(loss='categorical_crossentropy',\n",
    "                            metrics=['accuracy'],\n",
    "                            optimizer=Adam())\n",
    "    imgs, labels = dataset.training_set_num(number_of_samples=number_of_samples)\n",
    "    labels = to_categorical(labels, num_classes=num_classes)\n",
    "    # なぜepochsを30にしているかは書いてない。\n",
    "    mnist_classifier.fit(x=imgs, y=labels, batch_size=32, epochs=30, verbose=0)\n",
    "    evaluate_test(mnist_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 63us/step\n",
      "Test Accuracy: 57.41%\n"
     ]
    }
   ],
   "source": [
    "train_fully_supervised(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 63us/step\n",
      "Test Accuracy: 93.82%\n"
     ]
    }
   ],
   "source": [
    "train_fully_supervised(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 66us/step\n",
      "Test Accuracy: 92.65%\n"
     ]
    }
   ],
   "source": [
    "train_fully_supervised(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 67us/step\n",
      "Test Accuracy: 87.28%\n"
     ]
    }
   ],
   "source": [
    "train_fully_supervised(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
